{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ab496bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "run_id = '98c20fe21d2349a6bfefbe86d16a6e47'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84a69a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I know I shouldn't test with training data, this notebook is just for testing mlflow evaluation function.\n",
    "data = pd.read_csv('./train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525d0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_model = f'runs:/{run_id}/classifier'\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ee25010",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_logged_model = f'runs:/{run_id}/vectorizer'\n",
    "\n",
    "# Load model\n",
    "vec_loaded_model = mlflow.sklearn.load_model(vec_logged_model)\n",
    "\n",
    "data['vectorized_'] = vec_loaded_model.transform(data.trusted.values).toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8fda6142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/03/03 15:52:16 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2022/03/03 15:52:16 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as multiclass dataset, number of classes is inferred as 4\n",
      "2022/03/03 15:52:17 WARNING mlflow.models.evaluation.default_evaluator: SHAP or matplotlib package is not installed, so model explainability insights will not be logged.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1728x1152 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = mlflow.evaluate(loaded_model, \n",
    "                         data=data.vectorized_.tolist(), \n",
    "                         evaluators='default', \n",
    "                         targets=data.cluster.tolist(), \n",
    "                         model_type='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2003069b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9280442804428044, 'example_count': 542, 'f1_score_micro': 0.9280442804428044, 'f1_score_macro': 0.8915347651577761, 'log_loss': 2.4852625081208815}\n"
     ]
    }
   ],
   "source": [
    "print(result.metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
